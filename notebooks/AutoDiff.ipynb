{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, value, func='variable', parents=[], *args):\n",
    "        self.value = value\n",
    "        self.func = func\n",
    "        self.parents = parents\n",
    "        self.args = args\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'({self.value}, {self.func}, {self.args}, {self.parents})'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'({self.value}, {self.func}, {self.args}, {self.parents})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add(a: Node, b: Node):\n",
    "    # unbox\n",
    "    va = a.value\n",
    "    vb = b.value\n",
    "\n",
    "    # primitive\n",
    "    vc = np.add(va, vb)\n",
    "\n",
    "    # box\n",
    "    c = Node(vc, 'add', [a, b])\n",
    "    c.args = (va, vb)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def negative(a: Node):\n",
    "    # unbox\n",
    "    va = a.value\n",
    "\n",
    "    # primitive\n",
    "    vb = np.negative(va)\n",
    "\n",
    "    # box\n",
    "    b = Node(vb, 'negative', [a])\n",
    "    b.args = (va,)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def exp(a: Node):\n",
    "    # unbox\n",
    "    va = a.value\n",
    "\n",
    "    # primitive\n",
    "    vb = np.exp(va)\n",
    "\n",
    "    # box\n",
    "    b = Node(vb, 'exp', [a])\n",
    "    b.args = (va,)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reciprocal(a: Node):\n",
    "    # unbox\n",
    "    va = a.value\n",
    "\n",
    "    # primitive\n",
    "    vb = np.reciprocal(va)\n",
    "\n",
    "    # box\n",
    "    b = Node(vb, 'reciprocal', [a])\n",
    "    b.args = (va,)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def matmul(a: Node, b: Node):\n",
    "    va = a.value\n",
    "    vb = b.value\n",
    "\n",
    "    vc = va @ vb\n",
    "\n",
    "    c = Node(vc, 'matmul', [a, b])\n",
    "    c.args = (va, vb)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def relu(a: Node):\n",
    "    va = a.value\n",
    "\n",
    "    vc = va * (va > 0)\n",
    "\n",
    "    c = Node(vc, 'relu', [a])\n",
    "    c.args = (va,)\n",
    "    return c\n",
    "\n",
    "\n",
    "a = [1.0, -1.0]\n",
    "r_a = relu(Node(np.array(a))).value\n",
    "t_r_a = torch.relu(torch.tensor(a))\n",
    "np.testing.assert_array_equal(r_a, t_r_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def softmax(a: Node):\n",
    "    va = a.value\n",
    "\n",
    "    # this would be overflow!\n",
    "    # exp_va = np.exp(va)\n",
    "    # vc = exp_va / np.sum(exp_va)\n",
    "\n",
    "    shiftz = va - np.max(va)\n",
    "    exps = np.exp(shiftz)\n",
    "    vc = exps / np.sum(exps)\n",
    "\n",
    "    c = Node(vc, 'softmax', [a])\n",
    "    c.args = (va,)\n",
    "    return c\n",
    "\n",
    "\n",
    "x = [1., 2, 3]\n",
    "y = softmax(Node(np.array(x, dtype=np.float32)))\n",
    "t_x = torch.tensor(x, dtype=torch.float32)\n",
    "t_y = torch.softmax(t_x, dim=-1)\n",
    "# todo: why not equal? dtype is the cause\n",
    "np.testing.assert_allclose(y.value, t_y.cpu().detach().numpy())\n",
    "np.testing.assert_array_equal(y.value, t_y.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not equal\n\nMismatched elements: 1 / 1 (100%)\nMax absolute difference: 7.79850298e-08\nMax relative difference: 5.54026028e-08\n x: array(1.407606)\n y: array(1.407606, dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 24>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# todo: why not equal?\u001B[39;00m\n\u001B[1;32m     23\u001B[0m np\u001B[38;5;241m.\u001B[39mtesting\u001B[38;5;241m.\u001B[39massert_allclose(c\u001B[38;5;241m.\u001B[39mvalue, t_c\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy(), rtol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-7\u001B[39m)\n\u001B[0;32m---> 24\u001B[0m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtesting\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massert_array_equal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_c\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/AutoDiff/lib/python3.8/site-packages/numpy/testing/_private/utils.py:844\u001B[0m, in \u001B[0;36massert_array_compare\u001B[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001B[0m\n\u001B[1;32m    840\u001B[0m         err_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(remarks)\n\u001B[1;32m    841\u001B[0m         msg \u001B[38;5;241m=\u001B[39m build_err_msg([ox, oy], err_msg,\n\u001B[1;32m    842\u001B[0m                             verbose\u001B[38;5;241m=\u001B[39mverbose, header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[1;32m    843\u001B[0m                             names\u001B[38;5;241m=\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m'\u001B[39m), precision\u001B[38;5;241m=\u001B[39mprecision)\n\u001B[0;32m--> 844\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(msg)\n\u001B[1;32m    845\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    846\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtraceback\u001B[39;00m\n",
      "\u001B[0;31mAssertionError\u001B[0m: \nArrays are not equal\n\nMismatched elements: 1 / 1 (100%)\nMax absolute difference: 7.79850298e-08\nMax relative difference: 5.54026028e-08\n x: array(1.407606)\n y: array(1.407606, dtype=float32)"
     ]
    }
   ],
   "source": [
    "def cross_entropy(a: Node, b):\n",
    "    va = a.value\n",
    "\n",
    "    shiftz = va - np.max(va)\n",
    "    exps = np.exp(shiftz)\n",
    "    vc = -1 * np.log(exps[b] / np.sum(exps))\n",
    "\n",
    "    c = Node(vc, 'cross_entropy', [a])\n",
    "    c.args = (va, b)\n",
    "    return c\n",
    "\n",
    "\n",
    "va = [1., 2., 3.]\n",
    "vb = 1\n",
    "a = Node(np.array(va))\n",
    "c = cross_entropy(a, vb)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "t_a = torch.tensor(va, dtype=torch.float32)\n",
    "t_c = loss(t_a, torch.tensor(vb))\n",
    "\n",
    "# todo: why not equal?\n",
    "np.testing.assert_allclose(c.value, t_c.cpu().detach().numpy(), rtol=1e-7)\n",
    "np.testing.assert_array_equal(c.value, t_c.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def logistic(i):\n",
    "    return reciprocal(add(Node(1), exp(negative(i))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def backward_pass(g, end_node, vjps):\n",
    "    tmp_node = Node(end_node.value, parents=[end_node])\n",
    "    q = []\n",
    "    gs = {tmp_node: (g,)}\n",
    "    q.append(tmp_node)\n",
    "    while len(q) > 0:\n",
    "        cur_node = q.pop(0)\n",
    "        cur_gs = gs[cur_node]\n",
    "        for node, cur_g in zip(cur_node.parents, cur_gs):\n",
    "            q.append(node)\n",
    "            vjp = vjps[node.func]\n",
    "            grads = vjp(cur_g, node.value, *node.args)\n",
    "            if node not in gs:\n",
    "                gs[node] = grads\n",
    "            else:\n",
    "                gs[node] += grads\n",
    "    return gs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_vjp(g, ans, a, b):\n",
    "    return g, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def exp_vjp(g, ans, a):\n",
    "    return (ans * g,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def negative_vjp(g, ans, a):\n",
    "    return (-1 * g,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reciprocal_vjp(g, ans, a):\n",
    "    return (np.divide(-1, a * a) * g,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def variable_vjp(g, ans):\n",
    "    return (g,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vjps = {\n",
    "    \"add\": add_vjp,\n",
    "    \"negative\": negative_vjp,\n",
    "    \"exp\": exp_vjp,\n",
    "    \"reciprocal\": reciprocal_vjp,\n",
    "    \"variable\": variable_vjp,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def relu_vjp(g, ans, a):\n",
    "    return (ans * g,)\n",
    "\n",
    "\n",
    "vjps[\"relu\"] = relu_vjp\n",
    "a = [1., -1.]\n",
    "g = [1., 1.]\n",
    "x = Node(np.array(a))\n",
    "y = relu(x)\n",
    "t_x = torch.tensor(a, requires_grad=True)\n",
    "t_y = torch.relu(t_x)\n",
    "\n",
    "gs = backward_pass(np.array(g), y, vjps)\n",
    "t_y.backward(torch.tensor([1., 1.]))\n",
    "\n",
    "np.testing.assert_array_equal(gs[x][0], t_x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not equal\n\nMismatched elements: 2 / 3 (66.7%)\nMax absolute difference: 7.4505806e-09\nMax relative difference: 6.70552159e-08\n x: array([-0.111111, -0.111111,  0.222222])\n y: array([-0.111111, -0.111111,  0.222222], dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [19]\u001B[0m, in \u001B[0;36m<cell line: 30>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     26\u001B[0m gs \u001B[38;5;241m=\u001B[39m backward_pass(np\u001B[38;5;241m.\u001B[39marray(g, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32), y, vjps)\n\u001B[1;32m     28\u001B[0m np\u001B[38;5;241m.\u001B[39mtesting\u001B[38;5;241m.\u001B[39massert_allclose(gs[n_x][\u001B[38;5;241m0\u001B[39m], t_x\u001B[38;5;241m.\u001B[39mgrad, rtol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-7\u001B[39m)\n\u001B[0;32m---> 30\u001B[0m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtesting\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massert_array_equal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgs\u001B[49m\u001B[43m[\u001B[49m\u001B[43mn_x\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_x\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad\u001B[49m\u001B[43m)\u001B[49m\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/AutoDiff/lib/python3.8/site-packages/numpy/testing/_private/utils.py:844\u001B[0m, in \u001B[0;36massert_array_compare\u001B[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001B[0m\n\u001B[1;32m    840\u001B[0m         err_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(remarks)\n\u001B[1;32m    841\u001B[0m         msg \u001B[38;5;241m=\u001B[39m build_err_msg([ox, oy], err_msg,\n\u001B[1;32m    842\u001B[0m                             verbose\u001B[38;5;241m=\u001B[39mverbose, header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[1;32m    843\u001B[0m                             names\u001B[38;5;241m=\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m'\u001B[39m), precision\u001B[38;5;241m=\u001B[39mprecision)\n\u001B[0;32m--> 844\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(msg)\n\u001B[1;32m    845\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    846\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtraceback\u001B[39;00m\n",
      "\u001B[0;31mAssertionError\u001B[0m: \nArrays are not equal\n\nMismatched elements: 2 / 3 (66.7%)\nMax absolute difference: 7.4505806e-09\nMax relative difference: 6.70552159e-08\n x: array([-0.111111, -0.111111,  0.222222])\n y: array([-0.111111, -0.111111,  0.222222], dtype=float32)"
     ]
    }
   ],
   "source": [
    "# why? https://deepnotes.io/softmax-crossentropy\n",
    "def softmax_vjp(g, ans, a):\n",
    "    dim = len(ans)\n",
    "    t1 = ans.reshape(dim, 1)\n",
    "    t2 = ans.reshape(1, dim)\n",
    "    t3 = t1 @ t2\n",
    "    t4 = np.zeros((dim, dim))\n",
    "    for i in range(dim):\n",
    "        t4[i, i] = ans[i]\n",
    "    d = t4 - t3\n",
    "\n",
    "    return (d @ g,)\n",
    "\n",
    "\n",
    "vjps['softmax'] = softmax_vjp\n",
    "# x = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "# g = np.ones(10, dtype=np.float32)\n",
    "x = [1, 1, 1]\n",
    "g = [1., 1, 2]\n",
    "n_x = Node(np.array(x, dtype=np.float32))\n",
    "y = softmax(n_x)\n",
    "t_x = torch.tensor(x, requires_grad=True, dtype=torch.float32)\n",
    "t_y = torch.softmax(t_x, dim=-1)\n",
    "\n",
    "t_y.backward(torch.tensor(g))\n",
    "gs = backward_pass(np.array(g, dtype=np.float32), y, vjps)\n",
    "\n",
    "np.testing.assert_allclose(gs[n_x][0], t_x.grad, rtol=1e-7)\n",
    "\n",
    "np.testing.assert_array_equal(gs[n_x][0], t_x.grad)\n",
    "#assert np.abs(np.sum(gs[n_x][0] - t_x.grad.cpu().detach().numpy())) < 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not equal\n\nMismatched elements: 3 / 3 (100%)\nMax absolute difference: 3.4556622e-08\nMax relative difference: 8.42898928e-08\n x: array([ 0.090031, -0.755272,  0.665241])\n y: array([ 0.090031, -0.755271,  0.665241], dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [20]\u001B[0m, in \u001B[0;36m<cell line: 25>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     21\u001B[0m t_c\u001B[38;5;241m.\u001B[39mbackward(torch\u001B[38;5;241m.\u001B[39mtensor(vg))\n\u001B[1;32m     23\u001B[0m np\u001B[38;5;241m.\u001B[39mtesting\u001B[38;5;241m.\u001B[39massert_allclose(gs[a][\u001B[38;5;241m0\u001B[39m], t_a\u001B[38;5;241m.\u001B[39mgrad, rtol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-7\u001B[39m)\n\u001B[0;32m---> 25\u001B[0m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtesting\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massert_array_equal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgs\u001B[49m\u001B[43m[\u001B[49m\u001B[43ma\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_a\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad\u001B[49m\u001B[43m)\u001B[49m\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/AutoDiff/lib/python3.8/site-packages/numpy/testing/_private/utils.py:844\u001B[0m, in \u001B[0;36massert_array_compare\u001B[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001B[0m\n\u001B[1;32m    840\u001B[0m         err_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(remarks)\n\u001B[1;32m    841\u001B[0m         msg \u001B[38;5;241m=\u001B[39m build_err_msg([ox, oy], err_msg,\n\u001B[1;32m    842\u001B[0m                             verbose\u001B[38;5;241m=\u001B[39mverbose, header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[1;32m    843\u001B[0m                             names\u001B[38;5;241m=\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m'\u001B[39m), precision\u001B[38;5;241m=\u001B[39mprecision)\n\u001B[0;32m--> 844\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(msg)\n\u001B[1;32m    845\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    846\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtraceback\u001B[39;00m\n",
      "\u001B[0;31mAssertionError\u001B[0m: \nArrays are not equal\n\nMismatched elements: 3 / 3 (100%)\nMax absolute difference: 3.4556622e-08\nMax relative difference: 8.42898928e-08\n x: array([ 0.090031, -0.755272,  0.665241])\n y: array([ 0.090031, -0.755271,  0.665241], dtype=float32)"
     ]
    }
   ],
   "source": [
    "def cross_entropy_vjp(g, ans, a, b):\n",
    "    tmp_a = Node(a)\n",
    "    ps = softmax(tmp_a).value\n",
    "    ps[b] = ps[b] - 1\n",
    "\n",
    "    return (ps,)\n",
    "\n",
    "\n",
    "vjps['cross_entropy'] = cross_entropy_vjp\n",
    "va = [1., 2., 3.]\n",
    "vb = 1\n",
    "vg = 1.\n",
    "a = Node(np.array(va))\n",
    "c = cross_entropy(a, vb)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "t_a = torch.tensor(va, requires_grad=True)\n",
    "t_c = loss(t_a, torch.tensor(vb))\n",
    "\n",
    "gs = backward_pass(np.array(vg), c, vjps)\n",
    "t_c.backward(torch.tensor(vg))\n",
    "\n",
    "np.testing.assert_allclose(gs[a][0], t_a.grad, rtol=1e-7)\n",
    "\n",
    "np.testing.assert_array_equal(gs[a][0], t_a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def matmul_vjp(g, ans, a, b):\n",
    "    # a is a matrix\n",
    "    # b is a vector\n",
    "    b_d = np.matmul(np.transpose(a), g)\n",
    "    r, c = a.shape\n",
    "    # y=Wx\n",
    "    # dy/dW = [x;x;x] element wise multi g\n",
    "    a_d = np.multiply(np.tile(b, (r, 1)), g.reshape(r, 1))\n",
    "\n",
    "    return (a_d, b_d)\n",
    "\n",
    "\n",
    "vjps[\"matmul\"] = matmul_vjp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tz = torch.tensor([1.5, 1.5], requires_grad=True)\n",
    "# reciprocal(add(Node(1), exp(negative(i))))\n",
    "y = torch.reciprocal(torch.add(1., torch.exp(torch.negative(tz))))\n",
    "z = Node(np.array([1.5, 1.5]))\n",
    "logsit = logistic(z)\n",
    "\n",
    "y.backward(torch.tensor([1., 1.]))\n",
    "gs = backward_pass((1., 1.), logsit, vjps)\n",
    "\n",
    "assert np.sum(tz.grad.cpu().detach().numpy() - gs[z]) < 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t_x = torch.tensor([1.5, 1.6, 1.7], requires_grad=True)\n",
    "t_w = torch.tensor([\n",
    "    [1., 2, 3],\n",
    "    [4, 5, 6]\n",
    "], requires_grad=True)\n",
    "t_b = torch.tensor([0.3, 0.4], requires_grad=True)\n",
    "t_y = torch.matmul(t_w, t_x) + t_b\n",
    "t_y.backward(torch.tensor([1., 2.]))\n",
    "\n",
    "x = Node(np.array([1.5, 1.6, 1.7]))\n",
    "w = Node(np.array([\n",
    "    [1., 2, 3],\n",
    "    [4, 5, 6]\n",
    "]))\n",
    "b = Node(np.array([0.3, 0.4]))\n",
    "y = add(matmul(w, x), b)\n",
    "gs = backward_pass(np.array([1., 2.]), y, vjps)\n",
    "\n",
    "assert np.sum(t_x.grad.cpu().detach().numpy() - gs[x]) < 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t_x = torch.tensor([1.5, 1.6, -1700], requires_grad=True)\n",
    "t_w = torch.tensor([\n",
    "    [1., 2, 3],\n",
    "    [4, 5, 6]\n",
    "], requires_grad=True)\n",
    "t_b = torch.tensor([0.3, 0.4], requires_grad=True)\n",
    "t_y = torch.relu(torch.matmul(t_w, t_x) + t_b)\n",
    "t_y.backward(torch.tensor([1., 2.]))\n",
    "\n",
    "x = Node(np.array([1.5, 1.6, -1700]))\n",
    "w = Node(np.array([\n",
    "    [1., 2, 3],\n",
    "    [4, 5, 6]\n",
    "]))\n",
    "b = Node(np.array([0.3, 0.4]))\n",
    "y = relu(add(matmul(w, x), b))\n",
    "gs = backward_pass(np.array([1., 2.]), y, vjps)\n",
    "\n",
    "np.testing.assert_array_equal(t_x.grad, gs[x][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. single -> batch\n",
    "2. python -> cuda\n",
    "3. dy/dW and grad of softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def mlp(input: Node, hidden_dim=11):\n",
    "#     v_input = input.value\n",
    "#     # not support batch yet\n",
    "#     dim = len(v_input)\n",
    "#\n",
    "#     v_w1 = np.ones((hidden_dim, dim))\n",
    "#     w1 = Node(v_w1)\n",
    "#     v_b1 = np.ones(hidden_dim)\n",
    "#     b1 = Node(v_b1)\n",
    "#     h1 = relu(add(matmul(w1, input), b1))\n",
    "#     #print('h1: ',h1.value)\n",
    "#\n",
    "#     v_w2 = np.ones((10, hidden_dim))\n",
    "#     w2 = Node(v_w2)\n",
    "#     #print('w2:', w2)\n",
    "#     v_b2 = np.ones(10)\n",
    "#     b2 = Node(v_b2)\n",
    "#     output = softmax(add(matmul(w2, h1), b2))\n",
    "#     #print('output', output.value)\n",
    "#     return output, w1, b1, h1, w2, b2\n",
    "#\n",
    "#\n",
    "# def torch_mlp(input, hidden_dim=10):\n",
    "#     dim = len(input)\n",
    "#     #print(input)\n",
    "#     w1 = torch.autograd.Variable(torch.ones((hidden_dim, dim), dtype=torch.float), requires_grad=True)\n",
    "#     b1 = torch.autograd.Variable(torch.ones(hidden_dim, dtype=torch.float), requires_grad=True)\n",
    "#     h1 = torch.relu(torch.matmul(w1, input) + b1)\n",
    "#     #print('torch h1:',h1)\n",
    "#\n",
    "#     w2 = torch.autograd.Variable(torch.ones((10, hidden_dim), dtype=torch.float), requires_grad=True)\n",
    "#     #print('torch w2:', w2)\n",
    "#     b2 = torch.autograd.Variable(torch.ones(10, dtype=torch.float), requires_grad=True)\n",
    "#     output = torch.softmax(torch.matmul(w2, h1) + b2, dim=-1)\n",
    "#     #print('torch output:', output)\n",
    "#     return output, w1, b1, h1, w2, b2\n",
    "\n",
    "class Mlp(object):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        v_w1 = np.random.randn(hidden_dim, input_dim)\n",
    "        self.w1 = Node(v_w1)\n",
    "        v_b1 = np.random.randn(hidden_dim)\n",
    "        self.b1 = Node(v_b1)\n",
    "        v_w2 = np.random.randn(10, hidden_dim)\n",
    "        self.w2 = Node(v_w2)\n",
    "        #print('w2:', w2)\n",
    "        v_b2 = np.random.randn(10)\n",
    "        self.b2 = Node(v_b2)\n",
    "\n",
    "    def __call__(self, input):\n",
    "        h1 = relu(add(matmul(self.w1, input), self.b1))\n",
    "        output = softmax(add(matmul(self.w2, h1), self.b2))\n",
    "        return output\n",
    "\n",
    "    def update(self, gs):\n",
    "        learning_rate = 0.001\n",
    "        self.b2.value -= learning_rate * gs[self.b2][0]\n",
    "        self.w2.value -= learning_rate * gs[self.w2][0]\n",
    "        self.b1.value -= learning_rate * gs[self.b1][0]\n",
    "        self.w1.value -= learning_rate * gs[self.w1][0]\n",
    "\n",
    "\n",
    "# initialize with all ones won't work!\n",
    "class TorchMlp(object):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        self.w1 = torch.autograd.Variable(torch.randn((hidden_dim, input_dim), dtype=torch.float), requires_grad=True)\n",
    "        self.b1 = torch.autograd.Variable(torch.randn(hidden_dim, dtype=torch.float), requires_grad=True)\n",
    "        self.w2 = torch.autograd.Variable(torch.randn((10, hidden_dim), dtype=torch.float), requires_grad=True)\n",
    "        self.b2 = torch.autograd.Variable(torch.randn(10, dtype=torch.float), requires_grad=True)\n",
    "        self.cost_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = torch.relu(torch.matmul(self.w1, x) + self.b1)\n",
    "        output = torch.softmax(torch.matmul(self.w2, h1) + self.b2, dim=-1)\n",
    "        return output\n",
    "\n",
    "    def update(self):\n",
    "        learning_rate = 0.00001\n",
    "        self.b2.data -= learning_rate * self.b2.grad\n",
    "        self.w2.data -= learning_rate * self.w2.grad\n",
    "        self.b1.data -= learning_rate * self.b1.grad\n",
    "        self.w1.data -= learning_rate * self.w1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=1e-07, atol=0\n\nMismatched elements: 10 / 10 (100%)\nMax absolute difference: 1.\nMax relative difference: 9.70028294e+133\n x: array([0.000000e+00, 0.000000e+00, 3.499920e-28, 1.000000e+00,\n       6.492693e-31, 0.000000e+00, 1.602200e-31, 1.404348e-29,\n       2.123127e-18, 2.668722e-19], dtype=float32)\n y: array([1.000000e+000, 6.682237e-142, 9.438314e-114, 1.445316e-110,\n       6.693303e-165, 8.381899e-063, 2.139587e-137, 3.800375e-124,\n       4.257392e-110, 4.782700e-097])",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [39]\u001B[0m, in \u001B[0;36m<cell line: 20>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     17\u001B[0m t_y\u001B[38;5;241m.\u001B[39mbackward(torch\u001B[38;5;241m.\u001B[39mtensor(initial_g))\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# why  so big difference between the gradients\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtesting\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massert_allclose\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt_y\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-7\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m np\u001B[38;5;241m.\u001B[39mtesting\u001B[38;5;241m.\u001B[39massert_allclose(t_input\u001B[38;5;241m.\u001B[39mgrad, gs[\u001B[38;5;28minput\u001B[39m][\u001B[38;5;241m0\u001B[39m], rtol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-5\u001B[39m)\n\u001B[1;32m     23\u001B[0m np\u001B[38;5;241m.\u001B[39mtesting\u001B[38;5;241m.\u001B[39massert_array_equal(t_y\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy(), y\u001B[38;5;241m.\u001B[39mvalue)\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/AutoDiff/lib/python3.8/site-packages/numpy/testing/_private/utils.py:844\u001B[0m, in \u001B[0;36massert_array_compare\u001B[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001B[0m\n\u001B[1;32m    840\u001B[0m         err_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(remarks)\n\u001B[1;32m    841\u001B[0m         msg \u001B[38;5;241m=\u001B[39m build_err_msg([ox, oy], err_msg,\n\u001B[1;32m    842\u001B[0m                             verbose\u001B[38;5;241m=\u001B[39mverbose, header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[1;32m    843\u001B[0m                             names\u001B[38;5;241m=\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m'\u001B[39m), precision\u001B[38;5;241m=\u001B[39mprecision)\n\u001B[0;32m--> 844\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(msg)\n\u001B[1;32m    845\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    846\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtraceback\u001B[39;00m\n",
      "\u001B[0;31mAssertionError\u001B[0m: \nNot equal to tolerance rtol=1e-07, atol=0\n\nMismatched elements: 10 / 10 (100%)\nMax absolute difference: 1.\nMax relative difference: 9.70028294e+133\n x: array([0.000000e+00, 0.000000e+00, 3.499920e-28, 1.000000e+00,\n       6.492693e-31, 0.000000e+00, 1.602200e-31, 1.404348e-29,\n       2.123127e-18, 2.668722e-19], dtype=float32)\n y: array([1.000000e+000, 6.682237e-142, 9.438314e-114, 1.445316e-110,\n       6.693303e-165, 8.381899e-063, 2.139587e-137, 3.800375e-124,\n       4.257392e-110, 4.782700e-097])"
     ]
    }
   ],
   "source": [
    "input_len = 18\n",
    "hidden_dim = 11\n",
    "input_value = np.arange(0, input_len, dtype=np.float32)\n",
    "input = Node(input_value)\n",
    "mlp = Mlp(input_len, hidden_dim)\n",
    "y = mlp(input)\n",
    "#y, w1, b1, h1, w2, b2 = mlp(input)\n",
    "\n",
    "t_input = torch.tensor(input_value, requires_grad=True)\n",
    "torch_mlp = TorchMlp(input_len, hidden_dim)\n",
    "t_y = torch_mlp(t_input)\n",
    "#t_y, t_w1, t_b1, t_h1, t_w2, t_b2 = torch_mlp(t_input)\n",
    "\n",
    "# assert gradient\n",
    "initial_g = np.ones(10, dtype=np.float32)\n",
    "gs = backward_pass(initial_g, y, vjps)\n",
    "t_y.backward(torch.tensor(initial_g))\n",
    "\n",
    "# why  so big difference between the gradients\n",
    "np.testing.assert_allclose(t_y.cpu().detach().numpy(), y.value, rtol=1e-7)\n",
    "np.testing.assert_allclose(t_input.grad, gs[input][0], rtol=1e-5)\n",
    "\n",
    "np.testing.assert_array_equal(t_y.cpu().detach().numpy(), y.value)\n",
    "np.testing.assert_array_equal(t_input.grad, gs[input][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib import request\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "filename = [\n",
    "    [\"training_images\", \"train-images-idx3-ubyte.gz\"],\n",
    "    [\"test_images\", \"t10k-images-idx3-ubyte.gz\"],\n",
    "    [\"training_labels\", \"train-labels-idx1-ubyte.gz\"],\n",
    "    [\"test_labels\", \"t10k-labels-idx1-ubyte.gz\"]\n",
    "]\n",
    "\n",
    "\n",
    "def download_mnist():\n",
    "    base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "    for name in filename:\n",
    "        print(\"Downloading \" + name[1] + \"...\")\n",
    "        request.urlretrieve(base_url + name[1], name[1])\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "\n",
    "def save_mnist():\n",
    "    mnist = {}\n",
    "    for name in filename[:2]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28 * 28)\n",
    "    for name in filename[-2:]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    with open(\"mnist.pkl\", 'wb') as f:\n",
    "        pickle.dump(mnist, f)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "\n",
    "def init():\n",
    "    download_mnist()\n",
    "    save_mnist()\n",
    "\n",
    "\n",
    "def load():\n",
    "    with open(\"mnist.pkl\", 'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "Download complete.\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "init()\n",
    "x_train, y_train, x_test, y_test = load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Train torch MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.1384\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.1884\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.2418\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.2625\n",
      "loss tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.284\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.299\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3071\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3135\n",
      "loss tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3231\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3342\n",
      "loss tensor(2.4564, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3488\n",
      "loss tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3455\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3412\n",
      "loss tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3376\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3364\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3403\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3416\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3441\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3505\n",
      "loss tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3575\n",
      "loss tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3596\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3581\n",
      "loss tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3574\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3579\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3577\n",
      "loss tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3591\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3586\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3594\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3625\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3659\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3609\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3615\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3671\n",
      "loss tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3691\n",
      "loss tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3702\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3681\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.364\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.361\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3572\n",
      "loss tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.357\n",
      "loss tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3581\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3597\n",
      "loss tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3602\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3601\n",
      "loss tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3586\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3571\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3569\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3556\n",
      "loss tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3543\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3518\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.349\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3462\n",
      "loss tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3437\n",
      "loss tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3405\n",
      "loss tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3373\n",
      "loss tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3396\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.343\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3465\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3493\n",
      "loss tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
      "test acc:  0.3523\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    min = np.min(x)\n",
    "    max = np.max(x)\n",
    "    return (x - min) / (max - min)\n",
    "\n",
    "\n",
    "def cal_acc(preds, trues):\n",
    "    return np.sum(preds == trues) / len(preds)\n",
    "\n",
    "\n",
    "epoch_num = 1\n",
    "learning_rate = 1\n",
    "torch_mlp = TorchMlp(len(x_train[0]), 128)\n",
    "cost_func = nn.CrossEntropyLoss()\n",
    "losses = []\n",
    "normalized_x_train = normalize(x_train)\n",
    "normalized_x_test=normalize(x_test)\n",
    "for i in range(epoch_num):\n",
    "    print('epoch: ', i)\n",
    "    for j in range(len(y_train)):\n",
    "        ind = np.random.choice(len(y_train), 1)[0]\n",
    "        t_x = torch.tensor(normalized_x_train[ind], dtype=torch.float)\n",
    "        t_y = torch_mlp(t_x)\n",
    "        #print(t_y)\n",
    "        loss = cost_func(t_y, torch.tensor(y_train[ind]))\n",
    "        #print(loss)\n",
    "        # seems wrong? loss always reduce? maybe less than 0\n",
    "        loss.backward()\n",
    "        torch_mlp.update()\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        if j % 1000 == 999:\n",
    "            print('loss', loss)\n",
    "            losses.append(loss.cpu().detach().numpy())\n",
    "            preds = []\n",
    "            for x in normalized_x_test:\n",
    "                output = torch_mlp(torch.tensor(x, dtype=torch.float))\n",
    "                pred = torch.argmax(output).cpu().detach().numpy()\n",
    "                preds.append(pred)\n",
    "\n",
    "            acc = cal_acc(preds, y_test)\n",
    "            print('test acc: ', acc)\n",
    "            # print('train acc: ', train_acc)\n",
    "            # test_preds = torch_mlp(torch.tensor(x_test, dtype=torch.float))\n",
    "            # test_acc = accuracy_score(torch.argmax(test_preds, dim=1).cpu().detach().numpy(), y_test)\n",
    "            # print('test acc: ', test_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "loss 2.4611199590323865\n",
      "test acc:  0.104\n",
      "loss 2.4611501149060215\n",
      "test acc:  0.1144\n",
      "loss 2.4611501717202247\n",
      "test acc:  0.1476\n",
      "loss 2.461147882711231\n",
      "test acc:  0.1575\n",
      "loss 2.3963732048291693\n",
      "test acc:  0.1684\n",
      "loss 2.461150171734475\n",
      "test acc:  0.1853\n",
      "loss 1.4611501717344748\n",
      "test acc:  0.1972\n",
      "loss 2.461149134214046\n",
      "test acc:  0.2126\n",
      "loss 2.461150171734475\n",
      "test acc:  0.2266\n",
      "loss 2.4396076116421774\n",
      "test acc:  0.2441\n",
      "loss 1.461262969706156\n",
      "test acc:  0.2662\n",
      "loss 1.4611501718659807\n",
      "test acc:  0.2754\n",
      "loss 2.4611472095233426\n",
      "test acc:  0.2844\n",
      "loss 1.4611501717344748\n",
      "test acc:  0.2976\n",
      "loss 1.4611501717344748\n",
      "test acc:  0.3083\n",
      "loss 1.4611501717344964\n",
      "test acc:  0.3241\n",
      "loss 1.4611501717344797\n",
      "test acc:  0.3367\n",
      "loss 2.4611501666849778\n",
      "test acc:  0.3454\n",
      "loss 2.461150171734414\n",
      "test acc:  0.3606\n",
      "loss 1.4611501717344748\n",
      "test acc:  0.3624\n",
      "loss 2.461150166678513\n",
      "test acc:  0.3782\n",
      "loss 1.4611501718673654\n",
      "test acc:  0.3883\n",
      "loss 2.4172144703676848\n",
      "test acc:  0.39\n",
      "loss 1.4611501717344748\n",
      "test acc:  0.3995\n",
      "loss 1.4611501717344748\n",
      "test acc:  0.4093\n",
      "loss 1.4611501717344748\n",
      "test acc:  0.4144\n",
      "loss 1.465374081074833\n",
      "test acc:  0.4163\n",
      "loss 1.4611501717344748\n",
      "test acc:  0.4258\n",
      "loss 2.461150171734475\n",
      "test acc:  0.4267\n",
      "loss 1.4611501717344748\n",
      "test acc:  0.4311\n",
      "loss 1.4611501717358\n",
      "test acc:  0.4349\n",
      "loss 1.4611501717344748\n",
      "test acc:  0.4374\n",
      "loss 2.4611501717344746\n",
      "test acc:  0.4429\n",
      "loss 1.4611501717413353\n",
      "test acc:  0.4402\n",
      "loss 1.4611501776995839\n",
      "test acc:  0.4424\n",
      "loss 2.4046990368741024\n",
      "test acc:  0.4454\n",
      "loss 2.460984930121505\n",
      "test acc:  0.4521\n",
      "loss 2.461150171734475\n",
      "test acc:  0.4543\n",
      "loss 1.4611501717344748\n",
      "test acc:  0.4503\n",
      "loss 2.4607724237827426\n",
      "test acc:  0.4521\n",
      "loss 2.444346057027999\n",
      "test acc:  0.4546\n",
      "loss 1.4611501717344748\n",
      "test acc:  0.4601\n",
      "loss 1.4611501717344748\n",
      "test acc:  0.461\n",
      "loss 2.460072974289138\n",
      "test acc:  0.4567\n",
      "loss 2.461150171734475\n",
      "test acc:  0.4652\n",
      "loss 2.461150171734475\n",
      "test acc:  0.4681\n",
      "loss 2.461150171734475\n",
      "test acc:  0.4642\n",
      "loss 2.4611501717308966\n",
      "test acc:  0.4633\n",
      "loss 1.4611501717344748\n",
      "test acc:  0.4688\n",
      "loss 1.4611501717344748\n",
      "test acc:  0.4665\n",
      "loss 2.461150171734475\n",
      "test acc:  0.4722\n",
      "loss 2.459454410618153\n",
      "test acc:  0.4743\n",
      "loss 1.4611501717344748\n",
      "test acc:  0.4783\n",
      "loss 1.4611501717344748\n",
      "test acc:  0.4742\n",
      "loss 1.4611502019887952\n",
      "test acc:  0.478\n",
      "loss 2.461150171734475\n",
      "test acc:  0.4773\n",
      "loss 1.4611501717344748\n",
      "test acc:  0.48\n",
      "loss 2.461150171734475\n",
      "test acc:  0.4845\n",
      "loss 2.461150171734475\n",
      "test acc:  0.4862\n",
      "loss 1.4611501717344748\n",
      "test acc:  0.4866\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 1\n",
    "learning_rate = 1\n",
    "mlp = Mlp(len(x_train[0]), 128)\n",
    "cost_func = cross_entropy\n",
    "losses = []\n",
    "\n",
    "for i in range(epoch_num):\n",
    "    print('epoch: ', i)\n",
    "    for j in range(len(y_train)):\n",
    "        ind = np.random.choice(len(y_train), 1)[0]\n",
    "        x=Node(normalized_x_train[ind])\n",
    "        y = mlp(x)\n",
    "        #print(t_y)\n",
    "        loss = cost_func(y, y_train[ind])\n",
    "        #print(loss)\n",
    "        # seems wrong? loss always reduce? maybe less than 0\n",
    "        gs = backward_pass(np.array(1), loss, vjps)\n",
    "        mlp.update(gs)\n",
    "\n",
    "        if j % 1000 == 999:\n",
    "            print('loss', loss.value)\n",
    "            losses.append(loss.value)\n",
    "            preds = []\n",
    "            for x in normalized_x_test:\n",
    "                output = mlp(Node(x))\n",
    "                pred = np.argmax(output.value)\n",
    "                preds.append(pred)\n",
    "\n",
    "            acc = cal_acc(preds, y_test)\n",
    "            print('test acc: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "np.min(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch_mlp.b2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch_mlp.w2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TorchRealMlp(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.linear2(x)\n",
    "        return torch.softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "class MnistDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "torch_real_mlp = TorchRealMlp(len(x_train[0]))\n",
    "\n",
    "# for param in torch_real_mlp.parameters():\n",
    "#     param.data = nn.parameter.Parameter(torch.ones_like(param))\n",
    "for param in torch_real_mlp.parameters():\n",
    "    param.data = nn.parameter.Parameter(torch.randn_like(param))\n",
    "\n",
    "print(torch_real_mlp)\n",
    "cost_func = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(torch_real_mlp.parameters(), lr=0.01)\n",
    "losses = []\n",
    "learning_rate = 1\n",
    "epoch_num = 6\n",
    "batch_size = 1\n",
    "dataset = MnistDataset(x_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "normalized_x_train = normalize(x_train)  # must be normalized, or else not trainable\n",
    "# for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#     print(data)\n",
    "#     print(target)\n",
    "#     # Zero gradient buffers\n",
    "#     optimizer.zero_grad()\n",
    "#\n",
    "#     # Pass data through the network\n",
    "#     output = torch_real_mlp(data)\n",
    "#\n",
    "#     # Calculate loss\n",
    "#     loss = cost_func(output, target)\n",
    "#\n",
    "#     # Backpropagate\n",
    "#     loss.backward()\n",
    "#\n",
    "#     # Update weights\n",
    "#     optimizer.step()\n",
    "#\n",
    "#     if j % 1000 == 999:\n",
    "#         torch_real_mlp.eval()\n",
    "#         print('loss', loss)\n",
    "#         losses.append(loss.cpu().detach().numpy())\n",
    "#         train_acc = accuracy_score(torch.argmax(t_y, dim=1).cpu().detach().numpy(), y_train[ind])\n",
    "#         print('train acc: ', train_acc)\n",
    "#         test_preds = torch_real_mlp(torch.tensor(x_test, dtype=torch.float))\n",
    "#         test_acc = accuracy_score(torch.argmax(test_preds, dim=1).cpu().detach().numpy(), y_test)\n",
    "#         print('test acc: ', test_acc)\n",
    "\n",
    "\n",
    "for i in range(epoch_num):\n",
    "    for j in range(len(y_train) // batch_size):\n",
    "        torch_real_mlp.train()\n",
    "        optimizer.zero_grad()\n",
    "        ind = np.random.choice(len(y_train), batch_size)\n",
    "        t_x = torch.tensor(normalized_x_train[ind], dtype=torch.float)\n",
    "        t_y = torch_real_mlp(t_x)\n",
    "        labels = torch.tensor(y_train[ind])\n",
    "\n",
    "        loss = cost_func(t_y, labels)\n",
    "        #\n",
    "        # print('================begin=================', j)\n",
    "        # plt.imshow(x_train[ind].reshape(28,28))\n",
    "        # print(labels)\n",
    "        # print('label: ', y_train[ind])\n",
    "        # print('t_y: ', t_y)\n",
    "        # print('loss: ', loss)\n",
    "        # print('--------------------------------')\n",
    "        #seems wrong? loss always reduce? maybe less than 0\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if j % 100 == 99:\n",
    "        #     print('------------------------------')\n",
    "        #     t_y = torch_real_mlp(t_x)\n",
    "        #     print('t_y: ', t_y)\n",
    "        #     print('loss: ', cost_func(t_y, torch.tensor(y_train[ind])))\n",
    "        #     print('================end=================')\n",
    "        if j % 1000 == 999:\n",
    "            torch_real_mlp.eval()\n",
    "            print(f'loss{loss}/{i}', loss)\n",
    "            losses.append(loss.cpu().detach().numpy())\n",
    "            train_acc = accuracy_score(torch.argmax(t_y, dim=1).cpu().detach().numpy(), y_train[ind])\n",
    "            print('train acc: ', train_acc)\n",
    "            test_preds = torch_real_mlp(torch.tensor(x_test, dtype=torch.float))\n",
    "            test_acc = accuracy_score(torch.argmax(test_preds, dim=1).cpu().detach().numpy(), y_test)\n",
    "            print('test acc: ', test_acc)\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(torch.argmax(t_y, dim=1).cpu().detach().numpy(), y_train[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loss = nn.CrossEntropyLoss()\n",
    "# label = 4\n",
    "# y = torch.tensor([1.6835e-02, 2.7039e-04, 1.6209e-01, 2.7039e-04, 2.7039e-04, 2.7039e-04,\n",
    "#                   2.7039e-04, 1.0372e-03, 8.1841e-01, 2.7039e-04])\n",
    "# print(loss(y, torch.tensor(label)))\n",
    "# sy = torch.softmax(y, dim=-1)\n",
    "# -1 * torch.log(sy[label] / torch.sum(sy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}