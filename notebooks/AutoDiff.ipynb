{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, value, func='variable', parents=[], *args):\n",
    "        self.value = value\n",
    "        self.func = func\n",
    "        self.parents = parents\n",
    "        self.args = args\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'({self.value}, {self.func}, {self.args}, {self.parents})'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'({self.value}, {self.func}, {self.args}, {self.parents})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def add(a: Node, b: Node):\n",
    "    # unbox\n",
    "    va = a.value\n",
    "    vb = b.value\n",
    "\n",
    "    # primitive\n",
    "    vc = np.add(va, vb)\n",
    "\n",
    "    # box\n",
    "    c = Node(vc, 'add', [a, b])\n",
    "    c.args = (va, vb)\n",
    "    return c"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def negative(a: Node):\n",
    "    # unbox\n",
    "    va = a.value\n",
    "\n",
    "    # primitive\n",
    "    vb = np.negative(va)\n",
    "\n",
    "    # box\n",
    "    b = Node(vb, 'negative', [a])\n",
    "    b.args = (va,)\n",
    "    return b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def exp(a: Node):\n",
    "    # unbox\n",
    "    va = a.value\n",
    "\n",
    "    # primitive\n",
    "    vb = np.exp(va)\n",
    "\n",
    "    # box\n",
    "    b = Node(vb, 'exp', [a])\n",
    "    b.args = (va,)\n",
    "    return b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def reciprocal(a: Node):\n",
    "    # unbox\n",
    "    va = a.value\n",
    "\n",
    "    # primitive\n",
    "    vb = np.reciprocal(va)\n",
    "\n",
    "    # box\n",
    "    b = Node(vb, 'reciprocal', [a])\n",
    "    b.args = (va,)\n",
    "    return b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def matmul(a: Node, b: Node):\n",
    "    va = a.value\n",
    "    vb = b.value\n",
    "\n",
    "    vc = va @ vb\n",
    "\n",
    "    c = Node(vc, 'matmul', [a, b])\n",
    "    c.args = (va, vb)\n",
    "    return c"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def relu(a: Node):\n",
    "    va = a.value\n",
    "\n",
    "    vc = va * (va > 0)\n",
    "\n",
    "    c = Node(vc, 'relu', [a])\n",
    "    c.args = (va,)\n",
    "    return c\n",
    "\n",
    "\n",
    "a = [1.0, -1.0]\n",
    "r_a = relu(Node(np.array(a))).value\n",
    "t_r_a = torch.relu(torch.tensor(a))\n",
    "np.testing.assert_array_equal(r_a, t_r_a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "def softmax(a: Node):\n",
    "    va = a.value\n",
    "\n",
    "    # this would be overflow!\n",
    "    # exp_va = np.exp(va)\n",
    "    # vc = exp_va / np.sum(exp_va)\n",
    "\n",
    "    shiftz = va - np.max(va)\n",
    "    exps = np.exp(shiftz)\n",
    "    vc = exps / np.sum(exps)\n",
    "\n",
    "    c = Node(vc, 'softmax', [a])\n",
    "    c.args = (va,)\n",
    "    return c\n",
    "\n",
    "\n",
    "x = [1., 2, 3]\n",
    "y = softmax(Node(np.array(x)))\n",
    "t_x = torch.tensor(x, dtype=torch.float64)\n",
    "t_y = torch.softmax(t_x, dim=-1)\n",
    "# todo: why not equal?\n",
    "#np.testing.assert_array_equal(y.value, t_y.cpu().detach().numpy())\n",
    "assert np.abs(np.sum(y.value - t_y.cpu().detach().numpy())) < 0.0001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def logistic(i):\n",
    "    return reciprocal(add(Node(1), exp(negative(i))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\n",
    "def backward_pass(g, end_node, vjps):\n",
    "    tmp_node = Node(end_node.value, parents=[end_node])\n",
    "    q = []\n",
    "    gs = {tmp_node: (g,)}\n",
    "    q.append(tmp_node)\n",
    "    while len(q) > 0:\n",
    "        cur_node = q.pop(0)\n",
    "        cur_gs = gs[cur_node]\n",
    "        for node, cur_g in zip(cur_node.parents, cur_gs):\n",
    "            q.append(node)\n",
    "            vjp = vjps[node.func]\n",
    "            grads = vjp(cur_g, node.value, *node.args)\n",
    "            if node not in gs:\n",
    "                gs[node] = grads\n",
    "            else:\n",
    "                gs[node] += grads\n",
    "    return gs\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def add_vjp(g, ans, a, b):\n",
    "    return g, g"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def exp_vjp(g, ans, a):\n",
    "    return (ans * g,)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def negative_vjp(g, ans, a):\n",
    "    return (-1 * g,)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def reciprocal_vjp(g, ans, a):\n",
    "    return (np.divide(-1, a * a) * g,)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def variable_vjp(g, ans):\n",
    "    return (g,)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "vjps = {\n",
    "    \"add\": add_vjp,\n",
    "    \"negative\": negative_vjp,\n",
    "    \"exp\": exp_vjp,\n",
    "    \"reciprocal\": reciprocal_vjp,\n",
    "    \"variable\": variable_vjp,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def relu_vjp(g, ans, a):\n",
    "    return (ans * g,)\n",
    "\n",
    "\n",
    "vjps[\"relu\"] = relu_vjp\n",
    "a = [1., -1.]\n",
    "g = [1., 1.]\n",
    "x = Node(np.array(a))\n",
    "y = relu(x)\n",
    "t_x = torch.tensor(a, requires_grad=True)\n",
    "t_y = torch.relu(t_x)\n",
    "\n",
    "gs = backward_pass(np.array(g), y, vjps)\n",
    "t_y.backward(torch.tensor([1., 1.]))\n",
    "\n",
    "np.testing.assert_array_equal(gs[x][0], t_x.grad)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.46944695e-17 -3.46944695e-17 -3.46944695e-17 -3.46944695e-17\n",
      " -2.08166817e-17 -2.08166817e-17 -2.08166817e-17 -2.08166817e-17\n",
      " -2.08166817e-17 -2.08166817e-17]\n",
      "tensor([-1.1921e-08, -1.1921e-08, -1.1921e-08, -1.1921e-08, -1.1921e-08,\n",
      "        -1.1921e-08, -1.1921e-08, -1.1921e-08, -1.1921e-08, -1.1921e-08])\n"
     ]
    }
   ],
   "source": [
    "# why? https://deepnotes.io/softmax-crossentropy\n",
    "def softmax_vjp(g, ans, a):\n",
    "    t = ans.reshape(-1, 1)\n",
    "    jocbian = np.diagflat(t) - np.dot(t, t.T)\n",
    "    return (jocbian @ g,)\n",
    "\n",
    "\n",
    "vjps['softmax'] = softmax_vjp\n",
    "x = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "g = np.ones(10, dtype=np.float32)\n",
    "n_x = Node(np.array(x))\n",
    "y = softmax(n_x)\n",
    "t_x = torch.tensor(x, requires_grad=True)\n",
    "t_y = torch.softmax(t_x, dim=-1)\n",
    "\n",
    "t_y.backward(torch.tensor(g))\n",
    "gs = backward_pass(np.array(g), y, vjps)\n",
    "print(gs[n_x][0])\n",
    "print(t_x.grad)\n",
    "\n",
    "#np.testing.assert_array_equal(gs[n_x][0], t_x.grad)\n",
    "assert np.abs(np.sum(gs[n_x][0] - t_x.grad.cpu().detach().numpy())) < 0.0001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "0.08192506646547511"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.sum([0.09003057, 0.24472847,\n",
    "            0.66524096])\n",
    "b = 0.09003057 / a\n",
    "b - b * b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "-0.1628034023221312"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.24472847 * 0.66524096"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def matmul_vjp(g, ans, a, b):\n",
    "    # a is a matrix\n",
    "    # b is a vector\n",
    "    b_d = np.matmul(np.transpose(a), g)\n",
    "    r, c = a.shape\n",
    "    # y=Wx\n",
    "    # dy/dW = [x;x;x] element wise multi g\n",
    "    a_d = np.multiply(np.tile(b, (r, 1)), g.reshape(r, 1))\n",
    "\n",
    "    return (a_d, b_d)\n",
    "\n",
    "\n",
    "vjps[\"matmul\"] = matmul_vjp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tz = torch.tensor([1.5, 1.5], requires_grad=True)\n",
    "# reciprocal(add(Node(1), exp(negative(i))))\n",
    "y = torch.reciprocal(torch.add(1., torch.exp(torch.negative(tz))))\n",
    "z = Node(np.array([1.5, 1.5]))\n",
    "logsit = logistic(z)\n",
    "\n",
    "y.backward(torch.tensor([1., 1.]))\n",
    "gs = backward_pass((1., 1.), logsit, vjps)\n",
    "\n",
    "assert np.sum(tz.grad.cpu().detach().numpy() - gs[z]) < 0.00001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "t_x = torch.tensor([1.5, 1.6, 1.7], requires_grad=True)\n",
    "t_w = torch.tensor([\n",
    "    [1., 2, 3],\n",
    "    [4, 5, 6]\n",
    "], requires_grad=True)\n",
    "t_b = torch.tensor([0.3, 0.4], requires_grad=True)\n",
    "t_y = torch.matmul(t_w, t_x) + t_b\n",
    "t_y.backward(torch.tensor([1., 2.]))\n",
    "\n",
    "x = Node(np.array([1.5, 1.6, 1.7]))\n",
    "w = Node(np.array([\n",
    "    [1., 2, 3],\n",
    "    [4, 5, 6]\n",
    "]))\n",
    "b = Node(np.array([0.3, 0.4]))\n",
    "y = add(matmul(w, x), b)\n",
    "gs = backward_pass(np.array([1., 2.]), y, vjps)\n",
    "\n",
    "assert np.sum(t_x.grad.cpu().detach().numpy() - gs[x]) < 0.00001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "t_x = torch.tensor([1.5, 1.6, -1700], requires_grad=True)\n",
    "t_w = torch.tensor([\n",
    "    [1., 2, 3],\n",
    "    [4, 5, 6]\n",
    "], requires_grad=True)\n",
    "t_b = torch.tensor([0.3, 0.4], requires_grad=True)\n",
    "t_y = torch.relu(torch.matmul(t_w, t_x) + t_b)\n",
    "t_y.backward(torch.tensor([1., 2.]))\n",
    "\n",
    "x = Node(np.array([1.5, 1.6, -1700]))\n",
    "w = Node(np.array([\n",
    "    [1., 2, 3],\n",
    "    [4, 5, 6]\n",
    "]))\n",
    "b = Node(np.array([0.3, 0.4]))\n",
    "y = relu(add(matmul(w, x), b))\n",
    "gs = backward_pass(np.array([1., 2.]), y, vjps)\n",
    "\n",
    "np.testing.assert_array_equal(t_x.grad, gs[x][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. single -> batch\n",
    "2. python -> cuda\n",
    "3. dy/dW and grad of softmax"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "def mlp(input: Node, hidden_dim=11):\n",
    "    v_input = input.value\n",
    "    # not support batch yet\n",
    "    dim = len(v_input)\n",
    "\n",
    "    v_w1 = np.ones((hidden_dim, dim))\n",
    "    w1 = Node(v_w1)\n",
    "    v_b1 = np.ones(hidden_dim)\n",
    "    b1 = Node(v_b1)\n",
    "    h1 = relu(add(matmul(w1, input), b1))\n",
    "    #print('h1: ',h1.value)\n",
    "\n",
    "    v_w2 = np.ones((10, hidden_dim))\n",
    "    w2 = Node(v_w2)\n",
    "    #print('w2:', w2)\n",
    "    v_b2 = np.ones(10)\n",
    "    b2 = Node(v_b2)\n",
    "    output = softmax(add(matmul(w2, h1), b2))\n",
    "    #print('output', output.value)\n",
    "    return output, w1, b1, h1, w2, b2\n",
    "\n",
    "\n",
    "def torch_mlp(input, hidden_dim=10):\n",
    "    dim = len(input)\n",
    "\n",
    "    w1 = torch.ones((hidden_dim, dim), requires_grad=True)\n",
    "    b1 = torch.ones(hidden_dim, requires_grad=True)\n",
    "    h1 = torch.relu(torch.matmul(w1, input) + b1)\n",
    "    #print('torch h1:',h1)\n",
    "\n",
    "    w2 = torch.ones((10, hidden_dim), requires_grad=True)\n",
    "    #print('torch w2:', w2)\n",
    "    b2 = torch.ones(10, requires_grad=True)\n",
    "    output = torch.softmax(torch.matmul(w2, h1) + b2, dim=-1)\n",
    "    #print('torch output:', output)\n",
    "    return output, w1, b1, h1, w2, b2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-3.46944695e-17, -3.46944695e-17, -3.46944695e-17, -3.46944695e-17,\n",
      "       -2.08166817e-17, -2.08166817e-17, -2.08166817e-17, -2.08166817e-17,\n",
      "       -2.08166817e-17, -2.08166817e-17]),)\n",
      "tensor([-1.1921e-08, -1.1921e-08, -1.1921e-08, -1.1921e-08, -1.1921e-08,\n",
      "        -1.1921e-08, -1.1921e-08, -1.1921e-08, -1.1921e-08, -1.1921e-08])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not equal\n\nMismatched elements: 12 / 12 (100%)\nMax absolute difference: 1.19209281e-06\nMax relative difference: 6134352.64650432\n x: array([-1.192093e-06, -1.192093e-06, -1.192093e-06, -1.192093e-06,\n       -1.192093e-06, -1.192093e-06, -1.192093e-06, -1.192093e-06,\n       -1.192093e-06, -1.192093e-06, -1.192093e-06, -1.192093e-06],\n      dtype=float32)\n y: array([-1.943307e-13, -1.943307e-13, -1.943307e-13, -1.943307e-13,\n       -1.943307e-13, -1.943307e-13, -1.943307e-13, -1.943307e-13,\n       -1.943307e-13, -1.943307e-13, -1.943307e-13, -1.943307e-13])",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [107]\u001B[0m, in \u001B[0;36m<cell line: 19>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(gs[b2])\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(t_b2\u001B[38;5;241m.\u001B[39mgrad)\n\u001B[0;32m---> 19\u001B[0m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtesting\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massert_array_equal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt_input\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/AutoDiff/lib/python3.8/site-packages/numpy/testing/_private/utils.py:844\u001B[0m, in \u001B[0;36massert_array_compare\u001B[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001B[0m\n\u001B[1;32m    840\u001B[0m         err_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(remarks)\n\u001B[1;32m    841\u001B[0m         msg \u001B[38;5;241m=\u001B[39m build_err_msg([ox, oy], err_msg,\n\u001B[1;32m    842\u001B[0m                             verbose\u001B[38;5;241m=\u001B[39mverbose, header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[1;32m    843\u001B[0m                             names\u001B[38;5;241m=\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m'\u001B[39m), precision\u001B[38;5;241m=\u001B[39mprecision)\n\u001B[0;32m--> 844\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(msg)\n\u001B[1;32m    845\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    846\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtraceback\u001B[39;00m\n",
      "\u001B[0;31mAssertionError\u001B[0m: \nArrays are not equal\n\nMismatched elements: 12 / 12 (100%)\nMax absolute difference: 1.19209281e-06\nMax relative difference: 6134352.64650432\n x: array([-1.192093e-06, -1.192093e-06, -1.192093e-06, -1.192093e-06,\n       -1.192093e-06, -1.192093e-06, -1.192093e-06, -1.192093e-06,\n       -1.192093e-06, -1.192093e-06, -1.192093e-06, -1.192093e-06],\n      dtype=float32)\n y: array([-1.943307e-13, -1.943307e-13, -1.943307e-13, -1.943307e-13,\n       -1.943307e-13, -1.943307e-13, -1.943307e-13, -1.943307e-13,\n       -1.943307e-13, -1.943307e-13, -1.943307e-13, -1.943307e-13])"
     ]
    }
   ],
   "source": [
    "input_len = 12\n",
    "input_value = np.arange(0, input_len, dtype=np.float32)\n",
    "input = Node(input_value)\n",
    "y, w1, b1, h1, w2, b2 = mlp(input)\n",
    "\n",
    "t_input = torch.tensor(input_value, requires_grad=True)\n",
    "t_y, t_w1, t_b1, t_h1, t_w2, t_b2 = torch_mlp(t_input)\n",
    "\n",
    "assert np.abs(np.sum(t_y.cpu().detach().numpy() - y.value)) < 0.00001\n",
    "\n",
    "# assert gradient\n",
    "initial_g = np.ones(10, dtype=np.float32)\n",
    "gs = backward_pass(initial_g, y, vjps)\n",
    "t_y.backward(torch.tensor(initial_g))\n",
    "\n",
    "print(gs[b2])\n",
    "print(t_b2.grad)\n",
    "\n",
    "np.testing.assert_array_equal(t_input.grad, gs[input][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(Node(np.array([738., 738., 738., 738., 738., 738., 738., 738., 738., 738.]))).value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),)"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs[input]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "t_input = torch.tensor(np.arange(0, input_len), requires_grad=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "12"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t_input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}