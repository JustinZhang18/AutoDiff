{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, value, func='variable', parents=[], *args):\n",
    "        self.value = value\n",
    "        self.func = func\n",
    "        self.parents = parents\n",
    "        self.args = args\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'({self.value}, {self.func}, {self.args}, {self.parents})'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'({self.value}, {self.func}, {self.args}, {self.parents})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def add(a: Node, b: Node):\n",
    "    # unbox\n",
    "    va = a.value\n",
    "    vb = b.value\n",
    "\n",
    "    # primitive\n",
    "    vc = np.add(va, vb)\n",
    "\n",
    "    # box\n",
    "    c = Node(vc, 'add', [a, b])\n",
    "    c.args = (va, vb)\n",
    "    return c"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def negative(a: Node):\n",
    "    # unbox\n",
    "    va = a.value\n",
    "\n",
    "    # primitive\n",
    "    vb = np.negative(va)\n",
    "\n",
    "    # box\n",
    "    b = Node(vb, 'negative', [a])\n",
    "    b.args = (va,)\n",
    "    return b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def exp(a: Node):\n",
    "    # unbox\n",
    "    va = a.value\n",
    "\n",
    "    # primitive\n",
    "    vb = np.exp(va)\n",
    "\n",
    "    # box\n",
    "    b = Node(vb, 'exp', [a])\n",
    "    b.args = (va,)\n",
    "    return b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def reciprocal(a: Node):\n",
    "    # unbox\n",
    "    va = a.value\n",
    "\n",
    "    # primitive\n",
    "    vb = np.reciprocal(va)\n",
    "\n",
    "    # box\n",
    "    b = Node(vb, 'reciprocal', [a])\n",
    "    b.args = (va,)\n",
    "    return b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def matmul(a: Node, b: Node):\n",
    "    va = a.value\n",
    "    vb = b.value\n",
    "\n",
    "    vc = va @ vb\n",
    "\n",
    "    c = Node(vc, 'matmul', [a, b])\n",
    "    c.args = (va, vb)\n",
    "    return c"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def relu(a: Node):\n",
    "    va = a.value\n",
    "\n",
    "    vc = va * (va > 0)\n",
    "\n",
    "    c = Node(vc, 'relu', [a])\n",
    "    c.args = (va,)\n",
    "    return c\n",
    "\n",
    "\n",
    "a = [1.0, -1.0]\n",
    "r_a = relu(Node(np.array(a))).value\n",
    "t_r_a = torch.relu(torch.tensor(a))\n",
    "np.testing.assert_array_equal(r_a, t_r_a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "\n",
    "def softmax(a: Node):\n",
    "    va = a.value\n",
    "\n",
    "    # this would be overflow!\n",
    "    # exp_va = np.exp(va)\n",
    "    # vc = exp_va / np.sum(exp_va)\n",
    "\n",
    "    shiftz = va - np.max(va)\n",
    "    exps = np.exp(shiftz)\n",
    "    vc = exps / np.sum(exps)\n",
    "\n",
    "    c = Node(vc, 'softmax', [a])\n",
    "    c.args = (va,)\n",
    "    return c\n",
    "\n",
    "\n",
    "x = [1., 2, 3]\n",
    "y = softmax(Node(np.array(x, dtype=np.float32)))\n",
    "t_x = torch.tensor(x, dtype=torch.float32)\n",
    "t_y = torch.softmax(t_x, dim=-1)\n",
    "# todo: why not equal? dtype is the cause\n",
    "np.testing.assert_allclose(y.value, t_y.cpu().detach().numpy())\n",
    "np.testing.assert_array_equal(y.value, t_y.cpu().detach().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not equal\n\nMismatched elements: 1 / 1 (100%)\nMax absolute difference: 7.79850298e-08\nMax relative difference: 5.54026028e-08\n x: array(1.407606)\n y: array(1.407606, dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [131]\u001B[0m, in \u001B[0;36m<cell line: 24>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# todo: why not equal?\u001B[39;00m\n\u001B[1;32m     23\u001B[0m np\u001B[38;5;241m.\u001B[39mtesting\u001B[38;5;241m.\u001B[39massert_allclose(c\u001B[38;5;241m.\u001B[39mvalue, t_c\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy(), rtol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-7\u001B[39m)\n\u001B[0;32m---> 24\u001B[0m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtesting\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massert_array_equal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_c\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/AutoDiff/lib/python3.8/site-packages/numpy/testing/_private/utils.py:844\u001B[0m, in \u001B[0;36massert_array_compare\u001B[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001B[0m\n\u001B[1;32m    840\u001B[0m         err_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(remarks)\n\u001B[1;32m    841\u001B[0m         msg \u001B[38;5;241m=\u001B[39m build_err_msg([ox, oy], err_msg,\n\u001B[1;32m    842\u001B[0m                             verbose\u001B[38;5;241m=\u001B[39mverbose, header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[1;32m    843\u001B[0m                             names\u001B[38;5;241m=\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m'\u001B[39m), precision\u001B[38;5;241m=\u001B[39mprecision)\n\u001B[0;32m--> 844\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(msg)\n\u001B[1;32m    845\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    846\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtraceback\u001B[39;00m\n",
      "\u001B[0;31mAssertionError\u001B[0m: \nArrays are not equal\n\nMismatched elements: 1 / 1 (100%)\nMax absolute difference: 7.79850298e-08\nMax relative difference: 5.54026028e-08\n x: array(1.407606)\n y: array(1.407606, dtype=float32)"
     ]
    }
   ],
   "source": [
    "def cross_entropy(a: Node, b):\n",
    "    va = a.value\n",
    "\n",
    "    shiftz = va - np.max(va)\n",
    "    exps = np.exp(shiftz)\n",
    "    vc = -1 * np.log(exps[b] / np.sum(exps))\n",
    "\n",
    "    c = Node(vc, 'cross_entropy', [a])\n",
    "    c.args = (va, b)\n",
    "    return c\n",
    "\n",
    "\n",
    "va = [1., 2., 3.]\n",
    "vb = 1\n",
    "a = Node(np.array(va))\n",
    "c = cross_entropy(a, vb)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "t_a = torch.tensor(va, dtype=torch.float32)\n",
    "t_c = loss(t_a, torch.tensor(vb))\n",
    "\n",
    "# todo: why not equal?\n",
    "np.testing.assert_allclose(c.value, t_c.cpu().detach().numpy(), rtol=1e-7)\n",
    "np.testing.assert_array_equal(c.value, t_c.cpu().detach().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def logistic(i):\n",
    "    return reciprocal(add(Node(1), exp(negative(i))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "\n",
    "def backward_pass(g, end_node, vjps):\n",
    "    tmp_node = Node(end_node.value, parents=[end_node])\n",
    "    q = []\n",
    "    gs = {tmp_node: (g,)}\n",
    "    q.append(tmp_node)\n",
    "    while len(q) > 0:\n",
    "        cur_node = q.pop(0)\n",
    "        cur_gs = gs[cur_node]\n",
    "        for node, cur_g in zip(cur_node.parents, cur_gs):\n",
    "            q.append(node)\n",
    "            vjp = vjps[node.func]\n",
    "            grads = vjp(cur_g, node.value, *node.args)\n",
    "            if node not in gs:\n",
    "                gs[node] = grads\n",
    "            else:\n",
    "                gs[node] += grads\n",
    "    return gs\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def add_vjp(g, ans, a, b):\n",
    "    return g, g"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def exp_vjp(g, ans, a):\n",
    "    return (ans * g,)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def negative_vjp(g, ans, a):\n",
    "    return (-1 * g,)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def reciprocal_vjp(g, ans, a):\n",
    "    return (np.divide(-1, a * a) * g,)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def variable_vjp(g, ans):\n",
    "    return (g,)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "vjps = {\n",
    "    \"add\": add_vjp,\n",
    "    \"negative\": negative_vjp,\n",
    "    \"exp\": exp_vjp,\n",
    "    \"reciprocal\": reciprocal_vjp,\n",
    "    \"variable\": variable_vjp,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def relu_vjp(g, ans, a):\n",
    "    return (ans * g,)\n",
    "\n",
    "\n",
    "vjps[\"relu\"] = relu_vjp\n",
    "a = [1., -1.]\n",
    "g = [1., 1.]\n",
    "x = Node(np.array(a))\n",
    "y = relu(x)\n",
    "t_x = torch.tensor(a, requires_grad=True)\n",
    "t_y = torch.relu(t_x)\n",
    "\n",
    "gs = backward_pass(np.array(g), y, vjps)\n",
    "t_y.backward(torch.tensor([1., 1.]))\n",
    "\n",
    "np.testing.assert_array_equal(gs[x][0], t_x.grad)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not equal\n\nMismatched elements: 2 / 3 (66.7%)\nMax absolute difference: 7.4505806e-09\nMax relative difference: 6.70552159e-08\n x: array([-0.111111, -0.111111,  0.222222])\n y: array([-0.111111, -0.111111,  0.222222], dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [130]\u001B[0m, in \u001B[0;36m<cell line: 30>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     26\u001B[0m gs \u001B[38;5;241m=\u001B[39m backward_pass(np\u001B[38;5;241m.\u001B[39marray(g, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32), y, vjps)\n\u001B[1;32m     28\u001B[0m np\u001B[38;5;241m.\u001B[39mtesting\u001B[38;5;241m.\u001B[39massert_allclose(gs[n_x][\u001B[38;5;241m0\u001B[39m], t_x\u001B[38;5;241m.\u001B[39mgrad, rtol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-7\u001B[39m)\n\u001B[0;32m---> 30\u001B[0m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtesting\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massert_array_equal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgs\u001B[49m\u001B[43m[\u001B[49m\u001B[43mn_x\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_x\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad\u001B[49m\u001B[43m)\u001B[49m\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/AutoDiff/lib/python3.8/site-packages/numpy/testing/_private/utils.py:844\u001B[0m, in \u001B[0;36massert_array_compare\u001B[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001B[0m\n\u001B[1;32m    840\u001B[0m         err_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(remarks)\n\u001B[1;32m    841\u001B[0m         msg \u001B[38;5;241m=\u001B[39m build_err_msg([ox, oy], err_msg,\n\u001B[1;32m    842\u001B[0m                             verbose\u001B[38;5;241m=\u001B[39mverbose, header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[1;32m    843\u001B[0m                             names\u001B[38;5;241m=\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m'\u001B[39m), precision\u001B[38;5;241m=\u001B[39mprecision)\n\u001B[0;32m--> 844\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(msg)\n\u001B[1;32m    845\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    846\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtraceback\u001B[39;00m\n",
      "\u001B[0;31mAssertionError\u001B[0m: \nArrays are not equal\n\nMismatched elements: 2 / 3 (66.7%)\nMax absolute difference: 7.4505806e-09\nMax relative difference: 6.70552159e-08\n x: array([-0.111111, -0.111111,  0.222222])\n y: array([-0.111111, -0.111111,  0.222222], dtype=float32)"
     ]
    }
   ],
   "source": [
    "# why? https://deepnotes.io/softmax-crossentropy\n",
    "def softmax_vjp(g, ans, a):\n",
    "    dim = len(ans)\n",
    "    t1 = ans.reshape(dim, 1)\n",
    "    t2 = ans.reshape(1, dim)\n",
    "    t3 = t1 @ t2\n",
    "    t4 = np.zeros((dim, dim))\n",
    "    for i in range(dim):\n",
    "        t4[i, i] = ans[i]\n",
    "    d = t4 - t3\n",
    "\n",
    "    return (d @ g,)\n",
    "\n",
    "\n",
    "vjps['softmax'] = softmax_vjp\n",
    "# x = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "# g = np.ones(10, dtype=np.float32)\n",
    "x = [1, 1, 1]\n",
    "g = [1., 1, 2]\n",
    "n_x = Node(np.array(x, dtype=np.float32))\n",
    "y = softmax(n_x)\n",
    "t_x = torch.tensor(x, requires_grad=True, dtype=torch.float32)\n",
    "t_y = torch.softmax(t_x, dim=-1)\n",
    "\n",
    "t_y.backward(torch.tensor(g))\n",
    "gs = backward_pass(np.array(g, dtype=np.float32), y, vjps)\n",
    "\n",
    "np.testing.assert_allclose(gs[n_x][0], t_x.grad, rtol=1e-7)\n",
    "\n",
    "np.testing.assert_array_equal(gs[n_x][0], t_x.grad)\n",
    "#assert np.abs(np.sum(gs[n_x][0] - t_x.grad.cpu().detach().numpy())) < 0.0001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not equal\n\nMismatched elements: 3 / 3 (100%)\nMax absolute difference: 3.4556622e-08\nMax relative difference: 8.42898928e-08\n x: array([ 0.090031, -0.755272,  0.665241])\n y: array([ 0.090031, -0.755271,  0.665241], dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [202]\u001B[0m, in \u001B[0;36m<cell line: 25>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     21\u001B[0m t_c\u001B[38;5;241m.\u001B[39mbackward(torch\u001B[38;5;241m.\u001B[39mtensor(vg))\n\u001B[1;32m     23\u001B[0m np\u001B[38;5;241m.\u001B[39mtesting\u001B[38;5;241m.\u001B[39massert_allclose(gs[a][\u001B[38;5;241m0\u001B[39m], t_a\u001B[38;5;241m.\u001B[39mgrad, rtol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-7\u001B[39m)\n\u001B[0;32m---> 25\u001B[0m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtesting\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massert_array_equal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgs\u001B[49m\u001B[43m[\u001B[49m\u001B[43ma\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_a\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad\u001B[49m\u001B[43m)\u001B[49m\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/AutoDiff/lib/python3.8/site-packages/numpy/testing/_private/utils.py:844\u001B[0m, in \u001B[0;36massert_array_compare\u001B[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001B[0m\n\u001B[1;32m    840\u001B[0m         err_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(remarks)\n\u001B[1;32m    841\u001B[0m         msg \u001B[38;5;241m=\u001B[39m build_err_msg([ox, oy], err_msg,\n\u001B[1;32m    842\u001B[0m                             verbose\u001B[38;5;241m=\u001B[39mverbose, header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[1;32m    843\u001B[0m                             names\u001B[38;5;241m=\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m'\u001B[39m), precision\u001B[38;5;241m=\u001B[39mprecision)\n\u001B[0;32m--> 844\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(msg)\n\u001B[1;32m    845\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    846\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtraceback\u001B[39;00m\n",
      "\u001B[0;31mAssertionError\u001B[0m: \nArrays are not equal\n\nMismatched elements: 3 / 3 (100%)\nMax absolute difference: 3.4556622e-08\nMax relative difference: 8.42898928e-08\n x: array([ 0.090031, -0.755272,  0.665241])\n y: array([ 0.090031, -0.755271,  0.665241], dtype=float32)"
     ]
    }
   ],
   "source": [
    "def cross_entropy_vjp(g, ans, a, b):\n",
    "    tmp_a = Node(a)\n",
    "    ps = softmax(tmp_a).value\n",
    "    ps[b] = ps[b] - 1\n",
    "\n",
    "    return (ps,)\n",
    "\n",
    "\n",
    "vjps['cross_entropy'] = cross_entropy_vjp\n",
    "va = [1., 2., 3.]\n",
    "vb = 1\n",
    "vg = 1.\n",
    "a = Node(np.array(va))\n",
    "c = cross_entropy(a, vb)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "t_a = torch.tensor(va, requires_grad=True)\n",
    "t_c = loss(t_a, torch.tensor(vb))\n",
    "\n",
    "gs = backward_pass(np.array(vg), c, vjps)\n",
    "t_c.backward(torch.tensor(vg))\n",
    "\n",
    "np.testing.assert_allclose(gs[a][0], t_a.grad, rtol=1e-7)\n",
    "\n",
    "np.testing.assert_array_equal(gs[a][0], t_a.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def matmul_vjp(g, ans, a, b):\n",
    "    # a is a matrix\n",
    "    # b is a vector\n",
    "    b_d = np.matmul(np.transpose(a), g)\n",
    "    r, c = a.shape\n",
    "    # y=Wx\n",
    "    # dy/dW = [x;x;x] element wise multi g\n",
    "    a_d = np.multiply(np.tile(b, (r, 1)), g.reshape(r, 1))\n",
    "\n",
    "    return (a_d, b_d)\n",
    "\n",
    "\n",
    "vjps[\"matmul\"] = matmul_vjp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tz = torch.tensor([1.5, 1.5], requires_grad=True)\n",
    "# reciprocal(add(Node(1), exp(negative(i))))\n",
    "y = torch.reciprocal(torch.add(1., torch.exp(torch.negative(tz))))\n",
    "z = Node(np.array([1.5, 1.5]))\n",
    "logsit = logistic(z)\n",
    "\n",
    "y.backward(torch.tensor([1., 1.]))\n",
    "gs = backward_pass((1., 1.), logsit, vjps)\n",
    "\n",
    "assert np.sum(tz.grad.cpu().detach().numpy() - gs[z]) < 0.00001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "t_x = torch.tensor([1.5, 1.6, 1.7], requires_grad=True)\n",
    "t_w = torch.tensor([\n",
    "    [1., 2, 3],\n",
    "    [4, 5, 6]\n",
    "], requires_grad=True)\n",
    "t_b = torch.tensor([0.3, 0.4], requires_grad=True)\n",
    "t_y = torch.matmul(t_w, t_x) + t_b\n",
    "t_y.backward(torch.tensor([1., 2.]))\n",
    "\n",
    "x = Node(np.array([1.5, 1.6, 1.7]))\n",
    "w = Node(np.array([\n",
    "    [1., 2, 3],\n",
    "    [4, 5, 6]\n",
    "]))\n",
    "b = Node(np.array([0.3, 0.4]))\n",
    "y = add(matmul(w, x), b)\n",
    "gs = backward_pass(np.array([1., 2.]), y, vjps)\n",
    "\n",
    "assert np.sum(t_x.grad.cpu().detach().numpy() - gs[x]) < 0.00001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "t_x = torch.tensor([1.5, 1.6, -1700], requires_grad=True)\n",
    "t_w = torch.tensor([\n",
    "    [1., 2, 3],\n",
    "    [4, 5, 6]\n",
    "], requires_grad=True)\n",
    "t_b = torch.tensor([0.3, 0.4], requires_grad=True)\n",
    "t_y = torch.relu(torch.matmul(t_w, t_x) + t_b)\n",
    "t_y.backward(torch.tensor([1., 2.]))\n",
    "\n",
    "x = Node(np.array([1.5, 1.6, -1700]))\n",
    "w = Node(np.array([\n",
    "    [1., 2, 3],\n",
    "    [4, 5, 6]\n",
    "]))\n",
    "b = Node(np.array([0.3, 0.4]))\n",
    "y = relu(add(matmul(w, x), b))\n",
    "gs = backward_pass(np.array([1., 2.]), y, vjps)\n",
    "\n",
    "np.testing.assert_array_equal(t_x.grad, gs[x][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. single -> batch\n",
    "2. python -> cuda\n",
    "3. dy/dW and grad of softmax"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "def mlp(input: Node, hidden_dim=11):\n",
    "    v_input = input.value\n",
    "    # not support batch yet\n",
    "    dim = len(v_input)\n",
    "\n",
    "    v_w1 = np.ones((hidden_dim, dim))\n",
    "    w1 = Node(v_w1)\n",
    "    v_b1 = np.ones(hidden_dim)\n",
    "    b1 = Node(v_b1)\n",
    "    h1 = relu(add(matmul(w1, input), b1))\n",
    "    #print('h1: ',h1.value)\n",
    "\n",
    "    v_w2 = np.ones((10, hidden_dim))\n",
    "    w2 = Node(v_w2)\n",
    "    #print('w2:', w2)\n",
    "    v_b2 = np.ones(10)\n",
    "    b2 = Node(v_b2)\n",
    "    output = softmax(add(matmul(w2, h1), b2))\n",
    "    #print('output', output.value)\n",
    "    return output, w1, b1, h1, w2, b2\n",
    "\n",
    "\n",
    "def torch_mlp(input, hidden_dim=10):\n",
    "    dim = len(input)\n",
    "    #print(input)\n",
    "    w1 = torch.autograd.Variable(torch.ones((hidden_dim, dim), dtype=torch.float), requires_grad=True)\n",
    "    b1 = torch.autograd.Variable(torch.ones(hidden_dim, dtype=torch.float), requires_grad=True)\n",
    "    h1 = torch.relu(torch.matmul(w1, input) + b1)\n",
    "    #print('torch h1:',h1)\n",
    "\n",
    "    w2 = torch.autograd.Variable(torch.ones((10, hidden_dim), dtype=torch.float), requires_grad=True)\n",
    "    #print('torch w2:', w2)\n",
    "    b2 = torch.autograd.Variable(torch.ones(10, dtype=torch.float), requires_grad=True)\n",
    "    output = torch.softmax(torch.matmul(w2, h1) + b2, dim=-1)\n",
    "    #print('torch output:', output)\n",
    "    return output, w1, b1, h1, w2, b2\n",
    "\n",
    "# class Mlp(object):\n",
    "#     def __init__(self, input_dim, hidden_dim):\n",
    "#         v_w1 = np.ones((hidden_dim, input_dim))\n",
    "#         self.w1 = Node(v_w1)\n",
    "#         v_b1 = np.ones(hidden_dim)\n",
    "#         self.b1 = Node(v_b1)\n",
    "#         v_w2 = np.ones((10, hidden_dim))\n",
    "#         self.w2 = Node(v_w2)\n",
    "#         #print('w2:', w2)\n",
    "#         v_b2 = np.ones(10)\n",
    "#         self.b2 = Node(v_b2)\n",
    "#\n",
    "#     def __call__(self, input):\n",
    "#         h1 = relu(add(matmul(self.w1, input), self.b1))\n",
    "#         output = softmax(add(matmul(self.w2, h1), self.b2))\n",
    "#         return output\n",
    "#\n",
    "#     def update(self):\n",
    "#         pass\n",
    "#\n",
    "#\n",
    "# class TorchMlp(object):\n",
    "#     def __init__(self, input_dim, hidden_dim):\n",
    "#         self.w1 = torch.autograd.Variable(torch.ones((hidden_dim, input_dim), dtype=torch.float), requires_grad=True)\n",
    "#         self.b1 = torch.autograd.Variable(torch.ones(hidden_dim, dtype=torch.float), requires_grad=True)\n",
    "#         self.w2 = torch.autograd.Variable(torch.ones((10, hidden_dim), dtype=torch.float), requires_grad=True)\n",
    "#         self.b2 = torch.autograd.Variable(torch.ones(10, dtype=torch.float), requires_grad=True)\n",
    "#\n",
    "#     def __call__(self, input):\n",
    "#         h1 = torch.relu(torch.matmul(self.w1, input) + self.b1)\n",
    "#         output = torch.softmax(torch.matmul(self.w2, h1) + self.b2, dim=-1)\n",
    "#         return output\n",
    "#\n",
    "#     def update(self):\n",
    "#         pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=1e-05, atol=0\n\nMismatched elements: 18 / 18 (100%)\nMax absolute difference: 1.19209256e-06\nMax relative difference: 2668841.17088175\n x: array([-1.192093e-06, -1.192093e-06, -1.192093e-06, -1.192093e-06,\n       -1.192093e-06, -1.192093e-06, -1.192093e-06, -1.192093e-06,\n       -1.192093e-06, -1.192093e-06, -1.192093e-06, -1.192093e-06,...\n y: array([-4.466705e-13, -4.466705e-13, -4.466705e-13, -4.466705e-13,\n       -4.466705e-13, -4.466705e-13, -4.466705e-13, -4.466705e-13,\n       -4.466705e-13, -4.466705e-13, -4.466705e-13, -4.466705e-13,...",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [204]\u001B[0m, in \u001B[0;36m<cell line: 20>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     17\u001B[0m t_y\u001B[38;5;241m.\u001B[39mbackward(torch\u001B[38;5;241m.\u001B[39mtensor(initial_g))\n\u001B[1;32m     19\u001B[0m np\u001B[38;5;241m.\u001B[39mtesting\u001B[38;5;241m.\u001B[39massert_allclose(t_y\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy(), y\u001B[38;5;241m.\u001B[39mvalue, rtol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-7\u001B[39m)\n\u001B[0;32m---> 20\u001B[0m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtesting\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massert_allclose\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt_input\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m np\u001B[38;5;241m.\u001B[39mtesting\u001B[38;5;241m.\u001B[39massert_array_equal(t_y\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy(), y\u001B[38;5;241m.\u001B[39mvalue)\n\u001B[1;32m     23\u001B[0m np\u001B[38;5;241m.\u001B[39mtesting\u001B[38;5;241m.\u001B[39massert_array_equal(t_input\u001B[38;5;241m.\u001B[39mgrad, gs[\u001B[38;5;28minput\u001B[39m][\u001B[38;5;241m0\u001B[39m])\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/AutoDiff/lib/python3.8/site-packages/numpy/testing/_private/utils.py:844\u001B[0m, in \u001B[0;36massert_array_compare\u001B[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001B[0m\n\u001B[1;32m    840\u001B[0m         err_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(remarks)\n\u001B[1;32m    841\u001B[0m         msg \u001B[38;5;241m=\u001B[39m build_err_msg([ox, oy], err_msg,\n\u001B[1;32m    842\u001B[0m                             verbose\u001B[38;5;241m=\u001B[39mverbose, header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[1;32m    843\u001B[0m                             names\u001B[38;5;241m=\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m'\u001B[39m), precision\u001B[38;5;241m=\u001B[39mprecision)\n\u001B[0;32m--> 844\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(msg)\n\u001B[1;32m    845\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    846\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtraceback\u001B[39;00m\n",
      "\u001B[0;31mAssertionError\u001B[0m: \nNot equal to tolerance rtol=1e-05, atol=0\n\nMismatched elements: 18 / 18 (100%)\nMax absolute difference: 1.19209256e-06\nMax relative difference: 2668841.17088175\n x: array([-1.192093e-06, -1.192093e-06, -1.192093e-06, -1.192093e-06,\n       -1.192093e-06, -1.192093e-06, -1.192093e-06, -1.192093e-06,\n       -1.192093e-06, -1.192093e-06, -1.192093e-06, -1.192093e-06,...\n y: array([-4.466705e-13, -4.466705e-13, -4.466705e-13, -4.466705e-13,\n       -4.466705e-13, -4.466705e-13, -4.466705e-13, -4.466705e-13,\n       -4.466705e-13, -4.466705e-13, -4.466705e-13, -4.466705e-13,..."
     ]
    }
   ],
   "source": [
    "input_len = 18\n",
    "hidden_dim = 11\n",
    "input_value = np.arange(0, input_len, dtype=np.float32)\n",
    "input = Node(input_value)\n",
    "# mlp = Mlp(input_len, hidden_dim)\n",
    "# y = mlp(input)\n",
    "y, w1, b1, h1, w2, b2 = mlp(input)\n",
    "\n",
    "t_input = torch.tensor(input_value, requires_grad=True)\n",
    "# torch_mlp = TorchMlp(input_len, hidden_dim)\n",
    "# t_y = torch_mlp(t_input)\n",
    "t_y, t_w1, t_b1, t_h1, t_w2, t_b2 = torch_mlp(t_input)\n",
    "\n",
    "# assert gradient\n",
    "initial_g = np.ones(10, dtype=np.float32)\n",
    "gs = backward_pass(initial_g, y, vjps)\n",
    "t_y.backward(torch.tensor(initial_g))\n",
    "\n",
    "np.testing.assert_allclose(t_y.cpu().detach().numpy(), y.value, rtol=1e-7)\n",
    "np.testing.assert_allclose(t_input.grad, gs[input][0], rtol=1e-5)\n",
    "\n",
    "np.testing.assert_array_equal(t_y.cpu().detach().numpy(), y.value)\n",
    "np.testing.assert_array_equal(t_input.grad, gs[input][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib import request\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "filename = [\n",
    "    [\"training_images\", \"train-images-idx3-ubyte.gz\"],\n",
    "    [\"test_images\", \"t10k-images-idx3-ubyte.gz\"],\n",
    "    [\"training_labels\", \"train-labels-idx1-ubyte.gz\"],\n",
    "    [\"test_labels\", \"t10k-labels-idx1-ubyte.gz\"]\n",
    "]\n",
    "\n",
    "\n",
    "def download_mnist():\n",
    "    base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "    for name in filename:\n",
    "        print(\"Downloading \" + name[1] + \"...\")\n",
    "        request.urlretrieve(base_url + name[1], name[1])\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "\n",
    "def save_mnist():\n",
    "    mnist = {}\n",
    "    for name in filename[:2]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28 * 28)\n",
    "    for name in filename[-2:]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    with open(\"mnist.pkl\", 'wb') as f:\n",
    "        pickle.dump(mnist, f)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "\n",
    "def init():\n",
    "    download_mnist()\n",
    "    save_mnist()\n",
    "\n",
    "\n",
    "def load():\n",
    "    with open(\"mnist.pkl\", 'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "Download complete.\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "init()\n",
    "x_train, y_train, x_test, y_test = load()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train torch MLP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(10, 1)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([ 0.0010, -0.0090,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([-0.0090,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0090,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010, -0.0090,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0090,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010, -0.0090,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "        -0.0090,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 1.0090,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "        -0.0090,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 1.0090,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010, -0.0090,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0090,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010, -0.0090,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([-0.0090,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010, -0.0090])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        1.0090])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010, -0.0090,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010, -0.0090,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010, -0.0090])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        1.0090])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0090,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 1.0090, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010, -0.0090,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010, -0.0090,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010, -0.0090,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([-0.0090,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010, -0.0090])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        1.0090])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010, -0.0090,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "        -0.0090,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 1.0090,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010, -0.0090])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        1.0090])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0090,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 1.0090, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([-0.0090,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010, -0.0090,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([-0.0090,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010, -0.0090,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0090,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010, -0.0090,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0090,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010, -0.0090,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010, -0.0090,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0090,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 1.0090, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "        -0.0090,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 1.0090,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010, -0.0090,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0090,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0090,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 1.0090, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010, -0.0090])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        1.0090])\n",
      "grad:  tensor([-0.0090,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0090,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "        -0.0090,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 1.0090,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010, -0.0090,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([-0.0090,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([-0.0090,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([1.0090, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0090,  0.0010,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 1.0090, 0.9990, 0.9990,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,\n",
      "        -0.0090,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 1.0090,\n",
      "        0.9990])\n",
      "grad:  tensor([ 0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0090,\n",
      "         0.0010,  0.0010])\n",
      "data:  tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 1.0090, 0.9990,\n",
      "        0.9990])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [189]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      5\u001B[0m ind \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mchoice(\u001B[38;5;28mlen\u001B[39m(y_train), \u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m      6\u001B[0m t_x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(x_train[ind], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat)\n\u001B[0;32m----> 7\u001B[0m t_y, t_w1, t_b1, t_h1, t_w2, t_b2 \u001B[38;5;241m=\u001B[39m \u001B[43mtorch_mlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m cost_func \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[1;32m      9\u001B[0m loss \u001B[38;5;241m=\u001B[39m cost_func(t_y, torch\u001B[38;5;241m.\u001B[39mtensor(y_train[ind]))\n",
      "Input \u001B[0;32mIn [179]\u001B[0m, in \u001B[0;36mtorch_mlp\u001B[0;34m(input, hidden_dim)\u001B[0m\n\u001B[1;32m     24\u001B[0m dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28minput\u001B[39m)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m#print(input)\u001B[39;00m\n\u001B[0;32m---> 26\u001B[0m w1 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mVariable(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mones\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m)\u001B[49m, requires_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     27\u001B[0m b1 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mVariable(torch\u001B[38;5;241m.\u001B[39mones(hidden_dim, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat), requires_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     28\u001B[0m h1 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu(torch\u001B[38;5;241m.\u001B[39mmatmul(w1, \u001B[38;5;28minput\u001B[39m) \u001B[38;5;241m+\u001B[39m b1)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "epoch_num = 3\n",
    "learning_rate = 0.1\n",
    "t_y, t_w1, t_b1, t_h1, t_w2, t_b2 = torch_mlp(t_x, hidden_dim=128)\n",
    "for i in range(epoch_num):\n",
    "    for j in range(len(y_train)):\n",
    "        ind = np.random.choice(len(y_train), 1)[0]\n",
    "        t_x = torch.tensor(x_train[ind], dtype=torch.float)\n",
    "        cost_func = nn.CrossEntropyLoss()\n",
    "        loss = cost_func(t_y, torch.tensor(y_train[ind]))\n",
    "        loss.backward(torch.tensor(learning_rate))\n",
    "\n",
    "        t_b2.data -= t_b2.grad\n",
    "        t_w2.data -= t_w2.grad\n",
    "        t_b1.data -= t_b1.grad\n",
    "        t_w1.data -= t_w1.grad\n",
    "\n",
    "        if j % 100 == 99:\n",
    "            print(\"grad: \", t_b2.grad)\n",
    "            print(\"data: \", t_b2.data)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.4018, 0.2693, 0.3289]], grad_fn=<SoftmaxBackward0>)"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(input, dim=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1.3119)"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1 * torch.log(torch.tensor(0.2693))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "data": {
      "text/plain": "-0.7307"
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2693 - 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}