{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, value, func='variable', parents=[], *args):\n",
    "        self.value = value\n",
    "        self.func = func\n",
    "        self.parents = parents\n",
    "        self.args = args\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'({self.value}, {self.func}, {self.args}, {self.parents})'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'({self.value}, {self.func}, {self.args}, {self.parents})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def add(a: Node, b: Node):\n",
    "    # unbox\n",
    "    va = a.value\n",
    "    vb = b.value\n",
    "\n",
    "    # primitive\n",
    "    vc = np.add(va, vb)\n",
    "\n",
    "    # box\n",
    "    c = Node(vc, 'add', [a, b])\n",
    "    c.args = (va, vb)\n",
    "    return c"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def negative(a: Node):\n",
    "    # unbox\n",
    "    va = a.value\n",
    "\n",
    "    # primitive\n",
    "    vb = np.negative(va)\n",
    "\n",
    "    # box\n",
    "    b = Node(vb, 'negative', [a])\n",
    "    b.args = (va,)\n",
    "    return b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def exp(a: Node):\n",
    "    # unbox\n",
    "    va = a.value\n",
    "\n",
    "    # primitive\n",
    "    vb = np.exp(va)\n",
    "\n",
    "    # box\n",
    "    b = Node(vb, 'exp', [a])\n",
    "    b.args = (va,)\n",
    "    return b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def reciprocal(a: Node):\n",
    "    # unbox\n",
    "    va = a.value\n",
    "\n",
    "    # primitive\n",
    "    vb = np.reciprocal(va)\n",
    "\n",
    "    # box\n",
    "    b = Node(vb, 'reciprocal', [a])\n",
    "    b.args = (va,)\n",
    "    return b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def matmul(a: Node, b: Node):\n",
    "    va = a.value\n",
    "    vb = b.value\n",
    "\n",
    "    vc = va @ vb\n",
    "\n",
    "    c = Node(vc, 'matmul', [a, b])\n",
    "    c.args = (va, vb)\n",
    "    return c"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "def relu(a: Node):\n",
    "    va = a.value\n",
    "\n",
    "    vc = va * (va > 0)\n",
    "\n",
    "    c = Node(vc, 'relu', [a])\n",
    "    c.args = (va,)\n",
    "    return c\n",
    "\n",
    "\n",
    "a = [1.0, -1.0]\n",
    "r_a = relu(Node(np.array(a))).value\n",
    "t_r_a = torch.relu(torch.tensor(a))\n",
    "np.testing.assert_array_equal(r_a, t_r_a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "def softmax(a: Node):\n",
    "    va = a.value\n",
    "\n",
    "    exp_va = np.exp(va)\n",
    "    vc = exp_va / np.sum(exp_va)\n",
    "\n",
    "    c = Node(vc, 'softmax', [a])\n",
    "    c.args = (va,)\n",
    "    return c\n",
    "\n",
    "\n",
    "x = [1., 2, 3]\n",
    "y = softmax(Node(np.array(x)))\n",
    "t_x = torch.tensor(x, dtype=torch.float64)\n",
    "t_y = torch.softmax(t_x, dim=-1)\n",
    "# todo: why not equal?\n",
    "#np.testing.assert_array_equal(y.value, t_y.cpu().detach().numpy())\n",
    "assert np.abs(np.sum(y.value - t_y.cpu().detach().numpy())) < 0.0001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def logistic(i):\n",
    "    return reciprocal(add(Node(1), exp(negative(i))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "\n",
    "def backward_pass(g, end_node, vjps):\n",
    "    tmp_node = Node(end_node.value, parents=[end_node])\n",
    "    q = []\n",
    "    gs = {tmp_node: (g,)}\n",
    "    q.append(tmp_node)\n",
    "    while len(q) > 0:\n",
    "        cur_node = q.pop(0)\n",
    "        cur_gs = gs[cur_node]\n",
    "        for node, cur_g in zip(cur_node.parents, cur_gs):\n",
    "            q.append(node)\n",
    "            vjp = vjps[node.func]\n",
    "            grads = vjp(cur_g, node.value, *node.args)\n",
    "            if node not in gs:\n",
    "                gs[node] = grads\n",
    "            else:\n",
    "                gs[node] += grads\n",
    "    return gs\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def add_vjp(g, ans, a, b):\n",
    "    return g, g"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def exp_vjp(g, ans, a):\n",
    "    return (ans * g,)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def negative_vjp(g, ans, a):\n",
    "    return (-1 * g,)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def reciprocal_vjp(g, ans, a):\n",
    "    return (np.divide(-1, a * a) * g,)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def variable_vjp(g, ans):\n",
    "    return (g,)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "vjps = {\n",
    "    \"add\": add_vjp,\n",
    "    \"negative\": negative_vjp,\n",
    "    \"exp\": exp_vjp,\n",
    "    \"reciprocal\": reciprocal_vjp,\n",
    "    \"variable\": variable_vjp,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def relu_vjp(g, ans, a):\n",
    "    return (ans * g,)\n",
    "\n",
    "\n",
    "vjps[\"relu\"] = relu_vjp\n",
    "a = [1., -1.]\n",
    "g = [1., 1.]\n",
    "x = Node(np.array(a))\n",
    "y = relu(x)\n",
    "t_x = torch.tensor(a, requires_grad=True)\n",
    "t_y = torch.relu(t_x)\n",
    "\n",
    "gs = backward_pass(np.array(g), y, vjps)\n",
    "t_y.backward(torch.tensor([1., 1.]))\n",
    "\n",
    "np.testing.assert_array_equal(gs[x][0], t_x.grad)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "# why? https://deepnotes.io/softmax-crossentropy\n",
    "def softmax_vjp(g, ans, a):\n",
    "    t = ans.reshape(-1, 1)\n",
    "    jocbian = np.diagflat(t) - np.dot(t, t.T)\n",
    "    return (jocbian @ g,)\n",
    "\n",
    "\n",
    "vjps['softmax'] = softmax_vjp\n",
    "x = [1., 2]\n",
    "g = [1., 2.]\n",
    "n_x = Node(np.array(x))\n",
    "y = softmax(n_x)\n",
    "t_x = torch.tensor(x, requires_grad=True)\n",
    "t_y = torch.softmax(t_x, dim=-1)\n",
    "\n",
    "t_y.backward(torch.tensor(g))\n",
    "gs = backward_pass(np.array(g), y, vjps)\n",
    "\n",
    "#np.testing.assert_array_equal(gs[n_x][0], t_x.grad)\n",
    "assert np.abs(np.sum(gs[n_x][0] - t_x.grad.cpu().detach().numpy())) < 0.0001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "data": {
      "text/plain": "0.08192506646547511"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.sum([0.09003057, 0.24472847,\n",
    "            0.66524096])\n",
    "b = 0.09003057 / a\n",
    "b - b * b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "data": {
      "text/plain": "-0.1628034023221312"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.24472847 * 0.66524096"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def matmul_vjp(g, ans, a, b):\n",
    "    # a is a matrix\n",
    "    # b is a vector\n",
    "    b_d = np.matmul(np.transpose(a), g)\n",
    "    r, c = a.shape\n",
    "    # y=Wx\n",
    "    # dy/dW = [x;x;x] element wise multi g\n",
    "    a_d = np.multiply(np.tile(b, (r, 1)), g.reshape(r, 1))\n",
    "\n",
    "    return (a_d, b_d)\n",
    "\n",
    "\n",
    "vjps[\"matmul\"] = matmul_vjp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tz = torch.tensor([1.5, 1.5], requires_grad=True)\n",
    "# reciprocal(add(Node(1), exp(negative(i))))\n",
    "y = torch.reciprocal(torch.add(1., torch.exp(torch.negative(tz))))\n",
    "z = Node(np.array([1.5, 1.5]))\n",
    "logsit = logistic(z)\n",
    "\n",
    "y.backward(torch.tensor([1., 1.]))\n",
    "gs = backward_pass((1., 1.), logsit, vjps)\n",
    "\n",
    "assert np.sum(tz.grad.cpu().detach().numpy() - gs[z]) < 0.00001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "t_x = torch.tensor([1.5, 1.6, 1.7], requires_grad=True)\n",
    "t_w = torch.tensor([\n",
    "    [1., 2, 3],\n",
    "    [4, 5, 6]\n",
    "], requires_grad=True)\n",
    "t_b = torch.tensor([0.3, 0.4], requires_grad=True)\n",
    "t_y = torch.matmul(t_w, t_x) + t_b\n",
    "t_y.backward(torch.tensor([1., 2.]))\n",
    "\n",
    "x = Node(np.array([1.5, 1.6, 1.7]))\n",
    "w = Node(np.array([\n",
    "    [1., 2, 3],\n",
    "    [4, 5, 6]\n",
    "]))\n",
    "b = Node(np.array([0.3, 0.4]))\n",
    "y = add(matmul(w, x), b)\n",
    "gs = backward_pass(np.array([1., 2.]), y, vjps)\n",
    "\n",
    "assert np.sum(t_x.grad.cpu().detach().numpy() - gs[x]) < 0.00001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "t_x = torch.tensor([1.5, 1.6, -1700], requires_grad=True)\n",
    "t_w = torch.tensor([\n",
    "    [1., 2, 3],\n",
    "    [4, 5, 6]\n",
    "], requires_grad=True)\n",
    "t_b = torch.tensor([0.3, 0.4], requires_grad=True)\n",
    "t_y = torch.relu(torch.matmul(t_w, t_x) + t_b)\n",
    "t_y.backward(torch.tensor([1., 2.]))\n",
    "\n",
    "x = Node(np.array([1.5, 1.6, -1700]))\n",
    "w = Node(np.array([\n",
    "    [1., 2, 3],\n",
    "    [4, 5, 6]\n",
    "]))\n",
    "b = Node(np.array([0.3, 0.4]))\n",
    "y = relu(add(matmul(w, x), b))\n",
    "gs = backward_pass(np.array([1., 2.]), y, vjps)\n",
    "\n",
    "np.testing.assert_array_equal(t_x.grad, gs[x][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. single -> batch\n",
    "2. python -> cuda"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}